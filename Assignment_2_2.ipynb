{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Library Imports and Initial Setup**"
      ],
      "metadata": {
        "id": "nF4dkZFyk126"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from google.colab import drive\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "uP2GWpql2cZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the purpose of this exercise we used the Italian language from the Universal Dependencies treebanks. Specifically we used the Venice Italian Treebank (https://github.com/UniversalDependencies/UD_Italian-VIT/tree/master)."
      ],
      "metadata": {
        "id": "KGjI4WzQxAkH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing - Data Extraction**\n",
        "\n",
        "We read files in CoNLL-U format (common for linguistic data annotations) and extract sentences and their word annotations. The function also ensures that sentences are properly formatted by wrapping them with markers (< s > for start and < e > for end)."
      ],
      "metadata": {
        "id": "y8dF_2U7k9Qq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IyVpFmKF7FY"
      },
      "outputs": [],
      "source": [
        "# Function to add space after commas and periods if they are attached directly to words\n",
        "def add_space(sentence):\n",
        "    # Using regular expression to find comma or period attached to a word\n",
        "    sentence = re.sub(r'(\\w)([,.])', r'\\1 \\2', sentence)\n",
        "    return sentence\n",
        "\n",
        "# Function to read CoNLL-U formatted files and extract sentences and word annotations\n",
        "def read_conllu(file_path):\n",
        "    sentences = []\n",
        "    words = []\n",
        "    POS_tags = []\n",
        "    index = []\n",
        "    sentence = None\n",
        "\n",
        "    # Open the file and process it line by line\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        i = -1\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line == '':\n",
        "                if sentence:\n",
        "                    sentences.append(add_space(sentence))\n",
        "                    sentence = None\n",
        "            elif line.startswith('# text'):\n",
        "                i += 1\n",
        "                sentence = \"<s>\" + line.split(\"=\")[1] + \" <e>\"\n",
        "            elif line.startswith('#'):\n",
        "                continue\n",
        "            else:\n",
        "                tokens = line.split('\\t')\n",
        "                words.append(tokens[1])\n",
        "                index.append(i)\n",
        "                POS_tags.append(tokens[3])\n",
        "        sent_df = pd.DataFrame({'Sentences': sentences})\n",
        "        words_df = pd.DataFrame({'Words': words,'Sentence': index, 'POS': POS_tags})\n",
        "    return sent_df, words_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "# Set path for training data\n",
        "file_path_train = '/content/drive/My Drive/Colab Notebooks/it_vit-ud-train.conllu'\n",
        "train_sent_df, train_words_df = read_conllu(file_path_train)\n",
        "# Set paths for testing data\n",
        "file_path_test = '/content/drive/My Drive/Colab Notebooks/it_vit-ud-test.conllu'\n",
        "test_sent_df, test_words_df = read_conllu(file_path_test)\n",
        "# Set path for dev data\n",
        "file_path_dev = '/content/drive/My Drive/Colab Notebooks/it_vit-ud-dev.conllu'\n",
        "dev_sent_df, dev_words_df = read_conllu(file_path_dev)\n",
        "\n",
        "print(\"Train set Sentences: \\n\")\n",
        "print(train_sent_df,'\\n\\n')\n",
        "print(\"Train set Words: \\n\")\n",
        "print(train_words_df,'\\n\\n')\n",
        "\n",
        "print(\"Test set Sentences: \\n\")\n",
        "print(test_sent_df,'\\n\\n')\n",
        "print(\"Test set Words: \\n\")\n",
        "print(test_words_df,'\\n\\n')\n",
        "\n",
        "print(\"Dev set Sentences: \\n\")\n",
        "print(dev_sent_df,'\\n\\n')\n",
        "print(\"Dev set Words: \\n\")\n",
        "print(dev_words_df,'\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uvp7zJX-I1zB",
        "outputId": "c483a2e6-7bdb-4231-86d5-94df5b338f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Train set Sentences: \n",
            "\n",
            "                                              Sentences\n",
            "0     <s> Le infrastrutture come fattore di competit...\n",
            "1     <s> Negli ultimi anni la dinamica dei polo di ...\n",
            "2     <s> Il raggiungimento e il mantenimento di pos...\n",
            "3     <s> Quest'ultimo è funzione di variabili strut...\n",
            "4     <s> Il contesto milanese , se da un lato è sta...\n",
            "...                                                 ...\n",
            "8272                      <s> Premio Elsa Morante . <e>\n",
            "8273  <s> È nato il premio Elsa Morante che verrà as...\n",
            "8274  <s> Questo Premio che non avrà sede fissa né s...\n",
            "8275  <s> sono promotori dell'iniziativa Patrizia Ca...\n",
            "8276                           <s> Libri in campo . <e>\n",
            "\n",
            "[8277 rows x 1 columns] \n",
            "\n",
            "\n",
            "Train set Words: \n",
            "\n",
            "                 Words  Sentence    POS\n",
            "0                   Le         0    DET\n",
            "1       infrastrutture         0   NOUN\n",
            "2                 come         0    ADP\n",
            "3              fattore         0   NOUN\n",
            "4                   di         0    ADP\n",
            "...                ...       ...    ...\n",
            "241599               .      8275  PUNCT\n",
            "241600           Libri      8276   NOUN\n",
            "241601              in      8276    ADP\n",
            "241602           campo      8276   NOUN\n",
            "241603               .      8276  PUNCT\n",
            "\n",
            "[241604 rows x 3 columns] \n",
            "\n",
            "\n",
            "Test set Sentences: \n",
            "\n",
            "                                              Sentences\n",
            "0     <s> Non sono consentite assegnazioni provvisor...\n",
            "1     <s> È consentita inoltre la partecipazione pro...\n",
            "2     <s> I predetti motivi devono costituire oggett...\n",
            "3     <s> In caso di ricongiungimento al familiare d...\n",
            "4     <s> Ai fini della possibilità di presentazione...\n",
            "...                                                 ...\n",
            "1062  <s> Scrooge era il suo unico esecutore testame...\n",
            "1063  <s> Anzi il nostro Scrooge , che per verità il...\n",
            "1064  <s> Il ricordo dei funerali mi fa tornare al p...\n",
            "1065  <s> Non c'è dunque dubbio che Marley era morto...\n",
            "1066  <s> Questo mettiamolo bene in sodo , se no nie...\n",
            "\n",
            "[1067 rows x 1 columns] \n",
            "\n",
            "\n",
            "Test set Words: \n",
            "\n",
            "              Words  Sentence    POS\n",
            "0               Non         0    ADV\n",
            "1              sono         0    AUX\n",
            "2        consentite         0   VERB\n",
            "3      assegnazioni         0   NOUN\n",
            "4       provvisorie         0    ADJ\n",
            "...             ...       ...    ...\n",
            "27966           per      1066    ADP\n",
            "27967      narrarvi      1066      _\n",
            "27968        narrar      1066   VERB\n",
            "27969            vi      1066   PRON\n",
            "27970             .      1066  PUNCT\n",
            "\n",
            "[27971 rows x 3 columns] \n",
            "\n",
            "\n",
            "Dev set Sentences: \n",
            "\n",
            "                                             Sentences\n",
            "0     <s> Ha l'acqua calda , più o meno si veste . <e>\n",
            "1    <s> malgrado le guerre e i disastri naturali e...\n",
            "2    <s> È come un'energia che sta crescendo comple...\n",
            "3    <s> L'onorevole Charles Rose , deputato democr...\n",
            "4    <s> Da qualche tempo , la sua espressione pref...\n",
            "..                                                 ...\n",
            "738  <s> Le gravi esigenze di salute dell'aspirante...\n",
            "739  <s> Possono chiedere l'assegnazione provvisori...\n",
            "740  <s> La relativa domanda va formulata contestua...\n",
            "741  <s> Possono partecipare al movimento delle ass...\n",
            "742  <s> Gli insegnanti di cui al precedente art . ...\n",
            "\n",
            "[743 rows x 1 columns] \n",
            "\n",
            "\n",
            "Dev set Words: \n",
            "\n",
            "               Words  Sentence    POS\n",
            "0                 Ha         0   VERB\n",
            "1                 l'         0    DET\n",
            "2              acqua         0   NOUN\n",
            "3              calda         0    ADJ\n",
            "4                  ,         0  PUNCT\n",
            "...              ...       ...    ...\n",
            "31091        possono       742    AUX\n",
            "31092     richiedere       742   VERB\n",
            "31093            per       742    ADP\n",
            "31094  trasferimento       742   NOUN\n",
            "31095              .       742  PUNCT\n",
            "\n",
            "[31096 rows x 3 columns] \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Average Sentence Length (train set): \",train_sent_df['Sentences'].apply(lambda x: len(x.split())).mean())\n",
        "print(\"Number of Unique Words (train set): \",len(train_words_df[\"Words\"].unique()),\"\\n\")\n",
        "\n",
        "print(\"Average Sentence Length (test set): \",test_sent_df['Sentences'].apply(lambda x: len(x.split())).mean())\n",
        "print(\"Number of Unique Words (test set): \",len(test_words_df[\"Words\"].unique()),\"\\n\")\n",
        "\n",
        "print(\"Average Sentence Length (dev set): \",dev_sent_df['Sentences'].apply(lambda x: len(x.split())).mean())\n",
        "print(\"Number of Unique Words (dev set): \",len(dev_words_df[\"Words\"].unique()),\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT1HuFwbNtBl",
        "outputId": "65a035c5-140b-4fcc-9203-6a2a26f66126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Sentence Length (train set):  25.73082034553582\n",
            "Number of Unique Words (train set):  23045 \n",
            "\n",
            "Average Sentence Length (test set):  23.529522024367385\n",
            "Number of Unique Words (test set):  5850 \n",
            "\n",
            "Average Sentence Length (dev set):  35.51682368775236\n",
            "Number of Unique Words (dev set):  3637 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train Set POS Tags: \\n',train_words_df['POS'].unique(),'\\n')\n",
        "print('Number of Unique POS Tags (Train): ',len(train_words_df['POS'].unique()),'\\n\\n')\n",
        "print('Test Set POS Tags: \\n',test_words_df['POS'].unique(),'\\n')\n",
        "print('Number of Unique POS Tags (Test): ',len(test_words_df['POS'].unique()),'\\n\\n')\n",
        "print('Dev Set POS Tags: \\n',dev_words_df['POS'].unique(),'\\n')\n",
        "print('Number of Unique POS Tags (Dev): ',len(dev_words_df['POS'].unique()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03sYEXSpO8Mj",
        "outputId": "578b4af5-45ca-43e9-8435-99767f80e9b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Set POS Tags: \n",
            " ['DET' 'NOUN' 'ADP' 'PROPN' 'PUNCT' '_' 'ADJ' 'AUX' 'ADV' 'VERB' 'PRON'\n",
            " 'CCONJ' 'SCONJ' 'NUM' 'SYM' 'X' 'INTJ' 'PART'] \n",
            "\n",
            "Number of Unique POS Tags (Train):  18 \n",
            "\n",
            "\n",
            "Test Set POS Tags: \n",
            " ['ADV' 'AUX' 'VERB' 'NOUN' 'ADJ' '_' 'ADP' 'DET' 'PUNCT' 'CCONJ' 'PRON'\n",
            " 'NUM' 'SCONJ' 'PROPN' 'X' 'INTJ' 'SYM'] \n",
            "\n",
            "Number of Unique POS Tags (Test):  17 \n",
            "\n",
            "\n",
            "Dev Set POS Tags: \n",
            " ['VERB' 'DET' 'NOUN' 'ADJ' 'PUNCT' 'ADV' 'CCONJ' 'PRON' 'ADP' 'AUX' 'NUM'\n",
            " 'PROPN' '_' 'X' 'SCONJ' 'INTJ'] \n",
            "\n",
            "Number of Unique POS Tags (Dev):  16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Processing and DataFrame (Windows) Creation**\n",
        "\n",
        "We create new dataframes that represent \"windows\" around each word, capturing the local context of words. These dataframes are then saved as CSV files."
      ],
      "metadata": {
        "id": "51uXFS1kQ375"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Function to generate three-word combinations from a sentence\n",
        "def generate_three_word_combinations(sentence):\n",
        "    combinations = []\n",
        "    words = sentence.split()\n",
        "    for i in range(0,len(words)):\n",
        "        if (words[i] == \"<s>\") or (words[i] == \"<e>\"):\n",
        "            continue\n",
        "        combinations.append(list([words[i-1], words[i], words[i+1]]))\n",
        "    return combinations\n",
        "\n",
        "# Function to generate a dataframe from sentences and a word dataframe\n",
        "def generate_df(sent_df ,word_df,set_name):\n",
        "    new_data = {'Wi-1': [], 'Wi': [], 'Wi+1': [], \"Wi_POS_tag\" : []}\n",
        "    i = 0\n",
        "    for sentence in tqdm(sent_df['Sentences'], desc=\"Processing\" + set_name + \"Sentences\"):\n",
        "        combinations_list = generate_three_word_combinations(sentence)\n",
        "        for combination in combinations_list:\n",
        "            if (not word_df[(word_df[\"Words\"]==combination[1]) & (word_df[\"Sentence\"]==i)]['POS'].empty):\n",
        "                new_data['Wi-1'].append(combination[0])\n",
        "                new_data['Wi'].append(combination[1])\n",
        "                new_data['Wi+1'].append(combination[2])\n",
        "                new_data[\"Wi_POS_tag\"].append(word_df[(word_df[\"Words\"]==combination[1]) & (word_df[\"Sentence\"]==i)]['POS'].values[0])\n",
        "        i += 1\n",
        "    windows_df = pd.DataFrame(new_data)\n",
        "    return windows_df\n",
        "\n"
      ],
      "metadata": {
        "id": "PHqMWDZURBB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################################################################\n",
        "##  The following code uses the generate_df function on the data  ##\n",
        "##  to create new dataframes in a convenient form                 ##\n",
        "##  to use later during the training of our model.                ##\n",
        "##  We then save the dataframes as .csv files.                    ##\n",
        "####################################################################\n",
        "\n",
        "#train_windows_df = generate_df(train_sent_df, train_words_df, \"Train\")\n",
        "#test_windows_df = generate_df(test_sent_df, test_words_df, \"Test\")\n",
        "#dev_windows_df = generate_df(dev_sent_df, dev_words_df, \"Development\")\n",
        "\n",
        "#train_windows_df.to_csv(\"/content/drive/My Drive/Colab Notebooks/train_windows.csv\", index=False)\n",
        "#test_windows_df.to_csv(\"/content/drive/My Drive/Colab Notebooks/test_windows.csv\", index=False)\n",
        "#dev_windows_df.to_csv(\"/content/drive/My Drive/Colab Notebooks/dev_windows.csv\", index=False)"
      ],
      "metadata": {
        "id": "sfognhPWRKxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Baseline Model for POS Tagging**\n",
        "\n",
        "We have implememented a simple majority voting baseline model for POS tagging, where the most frequent POS tag for a word in the training set is used as the prediction for that word in the testing set. The function then evaluates the model using a classification report from sklearn."
      ],
      "metadata": {
        "id": "0KaERdt5laKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to implement a majority voting baseline for POS tagging\n",
        "def majority_baseline(train_df, test_df):\n",
        "    most_frequent_tags = train_df.groupby('Words')['POS'].agg(lambda x: x.mode().iloc[0]).to_dict()\n",
        "    test_df['prediction'] = test_df['Words'].map(most_frequent_tags)\n",
        "    most_frequent_tag_training = train_df['POS'].mode().iloc[0]\n",
        "    test_df['prediction'].fillna(most_frequent_tag_training, inplace=True)\n",
        "    report = classification_report(test_df['POS'], test_df['prediction'])\n",
        "    print(report)\n",
        "\n",
        "majority_baseline(train_words_df, test_words_df)"
      ],
      "metadata": {
        "id": "sT1y9uJ41TT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a810bf51-fca9-4ba8-dbfa-d0102852e2ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADJ       0.82      0.68      0.74      1565\n",
            "         ADP       0.99      0.99      0.99      3857\n",
            "         ADV       0.92      0.92      0.92      1258\n",
            "         AUX       0.86      0.97      0.91      1059\n",
            "       CCONJ       0.98      0.92      0.95       762\n",
            "         DET       0.94      0.98      0.96      3902\n",
            "        INTJ       0.69      0.69      0.69        16\n",
            "        NOUN       0.72      0.95      0.82      4962\n",
            "         NUM       0.98      0.86      0.91       418\n",
            "        PRON       0.79      0.78      0.79      1256\n",
            "       PROPN       0.98      0.55      0.71      1290\n",
            "       PUNCT       1.00      1.00      1.00      3351\n",
            "       SCONJ       0.84      0.40      0.54       320\n",
            "         SYM       1.00      1.00      1.00         3\n",
            "        VERB       0.93      0.63      0.75      2280\n",
            "           X       0.67      0.26      0.38        38\n",
            "           _       0.99      0.96      0.98      1634\n",
            "\n",
            "    accuracy                           0.89     27971\n",
            "   macro avg       0.89      0.80      0.83     27971\n",
            "weighted avg       0.90      0.89      0.89     27971\n",
            "\n"
          ]
        }
      ]
    }
  ]
}